{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d0e323",
   "metadata": {},
   "source": [
    "# Capstone project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df06cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bac5a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Release Date: November 29, 2002 [eBook #1661] [Most recently updated: May 20, 2019] Language: English Character set encoding: UTF-8 Produced by: an anonymous Project Gutenberg volunteer and Jose Menendez *** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES *** cover The Adventures of Sherlock Holmes by Arthur Conan Doyle Contents I. A Scandal in Bohemia II. The Red-Headed Leag'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"/Users/nitin/Desktop/AdventrueOfSherlockHolmes.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45bc2ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573327"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c9615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization: Tokenization refers to splitting bigger text data, essays, or corpus’s into smaller segments. \n",
    "#These smaller segments can be in the form of smaller documents or lines of text data.\n",
    "#They can also be a dictionary of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0919a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 141, 130, 857, 5, 1, 1060, 5, 127, 33, 46, 590, 2585, 2586, 27]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac348164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108907"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aa501bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8606\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c4b88b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  108904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1,  141,  130,  857],\n",
       "       [ 141,  130,  857,    5],\n",
       "       [ 130,  857,    5,    1],\n",
       "       [ 857,    5,    1, 1060],\n",
       "       [   5,    1, 1060,    5],\n",
       "       [   1, 1060,    5,  127],\n",
       "       [1060,    5,  127,   33],\n",
       "       [   5,  127,   33,   46],\n",
       "       [ 127,   33,   46,  590],\n",
       "       [  33,   46,  590, 2585]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "016ca9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8750fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[   1  141  130]\n",
      " [ 141  130  857]\n",
      " [ 130  857    5]\n",
      " [ 857    5    1]\n",
      " [   5    1 1060]\n",
      " [   1 1060    5]\n",
      " [1060    5  127]\n",
      " [   5  127   33]\n",
      " [ 127   33   46]\n",
      " [  33   46  590]]\n",
      "Response:  [ 857    5    1 1060    5  127   33   46  590 2585]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25274585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab879c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
